\documentclass[sigconf]{acmart}

\usepackage{booktabs} % For formal tables
\usepackage{graphicx}
\usepackage{xfrac}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage[toc,page]{appendix}

\newtheorem{problem}{Problem}


% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
%\acmDOI{10.475/123_4}

% ISBN
%\acmISBN{123-4567-24-567/08/06}

%Conference
%\acmConference[WOODSTOCK'97]{ACM Woodstock conference}{July 1997}{El
 % Paso, Texas USA}
%\acmYear{1997}
%\copyrightyear{2016}


%\acmArticle{4}
%\acmPrice{15.00}

% These commands are optional
%\acmBooktitle{Transactions of the ACM Woodstock conference}
%\editor{Jennifer B. Sartor}
%\editor{Theo D'Hondt}
%\editor{Wolfgang De Meuter}


\begin{document}
\title{Learning Features For Relational Data}


\author{Hoang Thanh Lam}
\affiliation{
  \institution{IBM Research}
  \streetaddress{IBM Technology Campus}
  \city{Dublin, Ireland}
 }
\email{t.l.hoang@ie.ibm.com}

\author{Tran Ngoc Minh}
\affiliation{
  \institution{IBM Research}
  \streetaddress{IBM Technology Campus}
  \city{Dublin, Ireland}
 }
\email{m.n.tran@ibm.com}

\author{Mathieu Sinn}
\affiliation{
  \institution{IBM Research}
  \streetaddress{IBM Technology Campus}
  \city{Dublin, Ireland}
 }
\email{mathsinn@ie.ibm.com}

\author{Beat Buesser}
\affiliation{
  \institution{IBM Research}
  \streetaddress{IBM Technology Campus}
  \city{Dublin, Ireland}
 }
\email{beat.buesser@ie.ibm.com}

\author{Martin Wistuba}
\affiliation{
  \institution{IBM Research}
  \streetaddress{IBM Technology Campus}
  \city{Dublin, Ireland}
 }
\email{martin.wistuba@ie.ibm.com}

% The default list of authors is too long for headers.
\renewcommand{\shortauthors}{Hoang Thanh Lam  et al.}


\begin{abstract}
Feature engineering is one of the most important but tedious tasks in data science projects. This work studies automation of feature learning for relational data. We first theoretically proved that  learning relevant features  from relational data for a given predictive analytics problem is NP-hard. However, it is possible to empirically show that an  efficient rule based approach   predefining   transformations  as a priori based on heuristics can extract very useful features from relational data. Indeed,  the proposed approach outperformed the state of the art solutions with a significant margin. We further introduce  a deep neural network  which automatically learns appropriate transformations of relational data into a representation that predicts the target variable well instead of being predefined as a priori by users. In an extensive experiment with  Kaggle competitions,  the proposed  methods could win late medals. To the best of our knowledge, this is the first time an automation system could win medals in Kaggle competitions with   complex relational data. 
\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below.
%
%\begin{CCSXML}
%<ccs2012>
 %<concept>
 % <concept_id>10010520.10010553.10010562</concept_id>
  %<concept_desc>Computer systems organization~Embedded systems</concept_desc>
  %<concept_significance>500</concept_significance>
 %</concept>
 %<concept>
  %<concept_id>10010520.10010575.10010755</concept_id>
 % <concept_desc>Computer systems organization~Redundancy</concept_desc>
  %<concept_significance>300</concept_significance>
 %</concept>
 %<concept>
  %<concept_id>10010520.10010553.10010554</concept_id>
  %<concept_desc>Computer systems organization~Robotics</concept_desc>
  %<concept_significance>100</concept_significance>
 %</concept>
 %<concept>
  %<concept_id>10003033.10003083.10003095</concept_id>
  %<concept_desc>Networks~Network reliability</concept_desc>
  %<concept_significance>100</concept_significance>
 %</concept>
%</ccs2012>
%\end{CCSXML}

%\ccsdesc[500]{Computer systems organization~Embedded systems}
%\ccsdesc[300]{Computer systems organization~Redundancy}
%\ccsdesc{Computer systems organization~Robotics}
%\ccsdesc[100]{Networks~Network reliability}


%\keywords{ACM proceedings, \LaTeX, text tagging}


\maketitle

\input{samplebody-conf}


\bibliographystyle{ACM-Reference-Format}
\bibliography{sigproc}

\pagebreak
\pagebreak
\newpage
\newpage
\begin{appendices}
\section{Proof of Lemma \ref{lemma:set function rnn} }
Denote $s$ as a set of numbers and $p(s)$ is any random permutation of $s$. A set function $f(s)$ is a map from any set $s$ to a real-value. Function $f$ is invariant with respect to set-permutation operation, i.e. $f(s) = f(p(s))$. For simplicity, we prove the lemma when the input is a set of scalar numbers. The general case for a set of vectors is proved in a similar way.


Consider the special case when $s = \{x_0, x_1\}$ and $p(s) = \{x_1, x_0\}$. According to definition of recurrent neural net we have:
\begin{eqnarray}
h_t &=& b + H*h_{t-1} + W*x_t \\ 
o_t &=& c + U*h_t
\end{eqnarray}

from which we have $rnn(s) = o_2$, where:
\begin{eqnarray}
h_1 &=& b + H*h_0 + W*x_0 \\
o_1 &=& c + U*h_1 \\
h_2 &=& b + H*h_1 + W*x_1 \\
o_2 &=& c + U*h_2 
\end{eqnarray}

In a similar way we can obtain the value of $rnn(p(s)) = o^*_2$, where:
\begin{eqnarray}
h^*_1 &=& b + H*h^*_0 + W*x_1 \\
o^*_1 &=& c + U*h^*_1 \\
h^*_2 &=& b + H*h^*_1 + W*x_0 \\
o^*_2 &=& c + U*h^*_2 
\end{eqnarray}
Since $rnn(p(s)) = rnn(s)$, we infer that:
\begin{eqnarray}
U*(H-1)*W *(x_0-x_1) = 0
\end{eqnarray}
The last equation holds for all value of $x_0, x_1$, therefore, either $H=1$, $W=0$ or $U=0$. The lemma is proved.
\section{Efficient implementation on GPU}
Deep learning techniques takes the advantage of fast matrix computation capabilities of GPU to speed up its training time. The speed-up is highly dependent upon whether or not the computation can be packed into a fixed size tensor before sending it to GPUs for massive parallel matrix computation. A problem with the network is that the structures of relational trees are different even for a given joining path. For instances, the relational trees in Figure \ref{exp:relational tree} have different structures depending on input data. This issue makes it difficult to normalize the computation across different relational trees in the same mini-batch to take the advantage of GPU computation. 

In this section, we discuss a computation normalization approach that allows speeding up the implementation 5x-10x using GPU computation under the assumption that the input to an r2n network are relational trees we set $\{D_{p_1}, D_{p_2}, \cdots, D_{p_q} \}$, where $D_{p_i} = \{t^{p_i}_1, t^{p_i}_2, \cdots, t^{p_i}_m\}$. 

It is important to notice that $t^{p_k}_i$ and $t^{p_l}_i$ have different structure when $p_l$ and $p_k$ are different. Therefore, normalization across joining paths is not a reasonable approach. For a given joining path $p_i$, the trees $t^{p_i}_k$ and $t^{p_i}_l$ in the set $D_{p_i}$ may have different structures as well. Fortunately, those trees share commons properties:
\begin{itemize}
\item they have the same maximum depth equal to the length of the path $p_i$
\item transformation based on RNN at each depth of the trees are shared
\end{itemize} 

Thanks to the common properties between the trees in $D_{p_i}$ the computation across the trees can be normalized. The input data at each depth of all the trees in $D_{p_i}$ (or a mini-batch) are transformed at once using the shared transformation network at the given depth. The output of the transformation is a list, for which we just need to identify which output corresponds to which tree for further transformation at the parent nodes of the trees.
 
 
\section{Model hyper-parameter tuning with Bayesian optimization }
The quality of the prediction highly depends on various hyper-parameters present in the machine learning method. Since their optimal choice highly depends on the data, fixing them abitrarily will provide suboptimal results. Therefore, we make use of Bayesian optimization \cite{Mockus1975}, a state-of-the-art approach for efficient and automated hyperparameter optimization \cite{Snoek2012}.

For Bayesian optimization, the problem of hyperparameter optimization is formulated as a black-box function minimization problem.  We define this black-box function $f$ by
\begin{equation}
f\ :\ \Lambda\rightarrow\mathbb{R}\enspace,
\end{equation}
where $f$ maps a hyperparameter configuration $\boldsymbol{\lambda}\in\Lambda$ to its loss on the validation data set. The evaluation of $f$ is a time-consuming step because it involves training our neural network on the training data set and evaluating it on the validation data set. However, minimizing $f$ will provide us the optimal hyperparameter configuration. 
\begin{equation}
\boldsymbol{\lambda}^{\ast}=\underset{\boldsymbol{\lambda}\in\Lambda}{\arg\,\min}f\left(\boldsymbol{\lambda}\right)\enspace.
\end{equation}

Bayesian optimization efficiently minimizes this expensive black-box function sequentially. In each optimization iteration the function $f$ is approximated by a Bayesian machine learning model, the surrogate model. We selected a Gaussian process with Mat\'{e}rn $\sfrac{5}{2}$ kernel as our surrogate model. This Bayesian model provides point and uncertainty estimates for the whole hyperparameter search space. According to a heuristic considering both mean and uncertainty prediction, the most promising hyperparameter configuration is evaluated next. We select expected improvement as our acquisition function \cite{Mockus1975}.
\label{sec:tuning}
\section{Parameter settings}
See Table \ref{tab:parameters}
\begin{table}[h]
  \small
  \begin{center}
  \caption {Parameter settings for OneBM and the r2n networks} \label{tab:parameters} 
  \begin{tabular}{ | l | c | }
   
    \hline
    \textbf{parameter} & \textbf{value} \\ \hline
    The number of recent values &  10  \\ \hline
    Maximum joined table size & $10^9$  \\ \hline
    The number of  highest correlated items & 10 \\ \hline
    Min correlation & $10^{-16}$ \\ \hline
    Min info-gain & $10^{-16}$ \\ \hline
    Optimization algorithm for backprop & ADAM \\ \hline
    Learning rate of ADAM & 0.01 \\ \hline
    Initial weights for FC and feed-forwards & Xavier \\ \hline
    Output size of FCs & 10 \\ \hline
    The number of  hidden layers in feedforward layers & 1 \\ \hline
    The number of  hidden layer size in feedforward layers & 1024 \\ \hline
	RNN cell &  LSTM \\ \hline
	LSTM cell size & 18 \\ \hline
	Max input sequence size & 50 \\ \hline
	Early  termination after no improvement on & 25\% training data \\ \hline
	Validation ratio & 10\% training data \\ \hline
  \end{tabular}
  \end{center}
\end{table}
\begin{figure*}[tb]
    \centering
    \includegraphics[width=1.0\textwidth]{./egraph_copy.pdf}
    \caption{Tweaked entity graphs of the Kaggle data used in our experiments for DFS}
    \label{tweak-database}
\end{figure*} 
\section{Baseline method settings}
DFS is  currently considered as the state of the art for automation of feature engineering for relational data. Recently,  DFS was  open-sourced\footnote{https://www.featuretools.com/} .  We compared OneBM and r2n to DFS (version 0.1.14). It is important to notice that the open-source version of DFS has been improved a lot since its first publication \cite{DFS}. For example, in the first version described in the publication there is no concept of temporal index which is very important to avoid mining leakages.

In order to use DFS properly, it requires knowledge about the data to create additional tables for interesting entities and to avoid  creating diamond entity graphs because DFS doesn't support diamond loops in the graph and does not allow many-many relations. The results of Grupo Bimbo and Coupon purchase competitions were reported using the open-source DFS after consulting with the authors on how to use DFS properly on these datasets.

For the Bimbo and Coupon purchased data, the entity graphs shown in  Figure \ref{fig:kaggle database} are not supported by DFS as they contain many-many relations and diamond subgraphs. Therefore, we  tweaked these graphs to let it run under the DFS framework. Particularly, for Bimbo data the relation between main and series tables is many-many. To go around this problem,  we created an additional table called product-client from the sale series table. Each entry in the new table   encodes the product, client pairs.  The product-client is the main table correspond to product-client pair. Since the competition asked for predicting sales of every pair of product-client at different time points, we created a cut-off time-stamp table, where each entry corresponds to exactly one  cut-off timestamp. The new entity graph is presented in Figure \ref{tweak-database}. We run DFS with maximum depth set to 1 and 2 and 3.

For Coupon datasets, more efforts are needed to prepare a proper input for DFS because the original entity graph contains both diamond loops and many-many relations. The latter issue can be resolved  by changing the connections as demonstrated in Figure \ref{tweak-database}. To avoid diamond loops we need to delete some relations. We decided to delete the relations (marked with an X) in Figure \ref{tweak-database}. Alternatively, we also tried to delete the relation between the  main and coupon-visit table but that led to much worse prediction than the given choice. 

In general, the results are highly dependant on how we use the tool so for the KDD cup 2014,  to have a fair comparison, we used the results in the original publication \cite{DFS}.  For KDD Cup 2015 and IJCAI 2015, the competitions are closed for submissions so  we couldn't report the results.

\section{Acknowledgements}
We would like to thank Johann Thiebaut, Dr. Tiep Mai, Dr. Bei Chen, Dr. Oznur Alkan, Dr. Olivier Verscheure, Dr. Eric Bouillet, Dr. Sean McKenna, Dr. Pol McAonghusa, Dr. Horst C. Samulowitz, Dr. Udayan Khurana and Tejaswina Pedapat for useful discussion and support during the development of the project.  We would like to thank Max Kanter for explaining us how to use DFS (featuretools) for the experiments.


\end{appendices}



\end{document}
